{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation Memory Retrieval using Weighted N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "from collections import Counter\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ashes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ilkncls csdnclsknc\n",
      "['ilkncls', 'csdnclsknc']\n"
     ]
    }
   ],
   "source": [
    "input_line = input()\n",
    "\n",
    "#convert input to lowercase\n",
    "input_line = input_line.lower()\n",
    "\n",
    "#tokenise\n",
    "input_tokens = word_tokenize(input_line)\n",
    "\n",
    "content_words = [word for word in input_tokens if word not in stop_words] #Removing Stopwords\n",
    "\n",
    "print(input_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted N-Gram Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_values = {}\n",
    "with open(\"../project/tm_data/tm_src.txt\") as source_file:\n",
    "    sentences = source_file.read().splitlines()\n",
    "\n",
    "\n",
    "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "for tkn in all_tokens_set:\n",
    "    contains_token = map(lambda doc: tkn in doc, tokenized_documents)\n",
    "    idf_values[tkn] = 1 + math.log(len(tokenized_documents)/(sum(contains_token)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the M_ngrams and C_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_M_ngrams():\n",
    "    sentence = \"There are a few controversies surrounding the the company may keep changing its business strategy topic how many songs did Rafi sing during his lifetime\"\n",
    "    ngrams_list_sent = []\n",
    "    M_ngrams = []\n",
    "    counter_ngrams = []\n",
    "    \n",
    "    ngrams = list(nltk.ngrams(sentence.split(), 4))\n",
    "    ngrams_list_sent.append(list(ngrams))\n",
    "    M_ngrams = [y for x in ngrams_list_sent for y in x]\n",
    "    \n",
    "    for ngrams in M_ngrams:\n",
    "        counter_ngrams.append(Counter(ngrams))\n",
    "        \n",
    "    return M_ngrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_ngrams = get_M_ngrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_C_ngrams(candidate_sentence):\n",
    "    ngrams_list_sent = []\n",
    "    C_ngrams = []\n",
    "    counter_ngrams = []\n",
    "    \n",
    "    ngrams = list(nltk.ngrams(candidate_sentence.split(), 4))\n",
    "    ngrams_list_sent.append(list(ngrams))\n",
    "    C_ngrams = [y for x in ngrams_list_sent for y in x]\n",
    "    ngrams_sents = []\n",
    "    ngrams_list_sent = []\n",
    "    \n",
    "    for ngrams in C_ngrams:\n",
    "        counter_ngrams.append(Counter(ngrams))\n",
    "    \n",
    "    return C_ngrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To compute numerator and denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_intersection(candidate_sentence):\n",
    "    C_ngrams = get_C_ngrams(candidate_sentence)\n",
    "    \n",
    "    M_set = set(M_ngrams)\n",
    "    C_set = set(C_ngrams)\n",
    "    \n",
    "    return list(M_set & C_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_w_sum(ngrams_list):\n",
    "    w = 0\n",
    "    \n",
    "    for ngram in ngrams_list:\n",
    "        for token in ngram:\n",
    "            if token in idf_values:\n",
    "                w+= idf_values[token] \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final score for each sentence wrt to input sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wpn(candidate_sentence):\n",
    "    C_ngrams = get_C_ngrams(candidate_sentence)\n",
    "    intersection_ngrams = ngrams_intersection(candidate_sentence)\n",
    "    Z = 0.75\n",
    "    \n",
    "    w_M_ngrams = compute_w_sum(M_ngrams)\n",
    "    w_C_ngrams = compute_w_sum(C_ngrams)\n",
    "    w_intersection_ngrams = compute_w_sum(intersection_ngrams)\n",
    "    \n",
    "    \n",
    "    wpn = w_intersection_ngrams / ((Z*w_M_ngrams) + ((1-Z)*w_C_ngrams))\n",
    "    \n",
    "    return wpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are a few societies 0.04892725559201201\n",
      "There are a few societies 0.04892725559201201\n",
      "The company may keep changing its business strategy in a steady pace to adapt to the pressure and competition. 0.21756950755179016\n",
      "The company may keep changing its business strategy in a steady pace to adapt to the pressure and competition. 0.21756950755179016\n",
      "There are a few controversies surrounding the topic how many songs did Rafi sing during his lifetime . 0.5544249929472864\n"
     ]
    }
   ],
   "source": [
    "max_wpn = 0\n",
    "wnp_all = []\n",
    "N = 5\n",
    "\n",
    "for sentence in sentences:\n",
    "    wpn = compute_wpn(sentence)\n",
    "    wnp_all.append(wpn)\n",
    "    if wpn > max_wpn:\n",
    "        max_wpn = wpn\n",
    "        best_sentence = sentence\n",
    "            \n",
    "        \n",
    "wnp_all = np.array(wnp_all)\n",
    "sorted_indices = np.argsort(wnp_all) \n",
    "least_N_indices = sorted_indices[-N:] \n",
    "\n",
    "print()\n",
    "for i in least_N_indices:\n",
    "    print(sentences[i], wnp_all[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval of Target from TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[756457] आपके नौकरी के आवेदन को कैसे परखेगा.\n",
      "\n",
      "[708745] रेफ डॉक:  IIT(BHU/IWD/ET/04/Hostel/2018 -19/813-\n",
      "\n",
      "[769884] उसका मन तो एक ही बात में लगा था कि जो कुछ भी उसने बीते सालों में सीखा है उस सबको एक बार दोहरा ले ।<s> वह कीमियागर जरूर उसकी परीक्षा लेगा ।\n",
      "\n",
      "[3] कंपनी दबाव और प्रतिस्पर्धा के अनुकूल बनने के लिए धीमी गति से अपनी व्यावसायिक रणनीति को बदल सकती है।\n",
      "\n",
      "[9] रफ़ी ने अपने जीवन में कुल कितने गाने गाए इस पर कुछ विवाद है ।\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tgt_tm_array = []\n",
    "\n",
    "with open('../project/tm_data/tm_tgt.txt') as tgt_tm:\n",
    "    line = tgt_tm.readline()\n",
    "    \n",
    "    while line:\n",
    "        tgt_tm_array.append(line)\n",
    "        line = tgt_tm.readline()\n",
    "  \n",
    "    for i in least_N_indices:\n",
    "        print([i], tgt_tm_array[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
