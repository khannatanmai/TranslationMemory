{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation Memory Retrieval using Weighted N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "from collections import Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ashes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please remove the drive safely\n",
      "['please', 'remove', 'drive', 'safely']\n"
     ]
    }
   ],
   "source": [
    "input_line = input()\n",
    "\n",
    "#convert input to lowercase\n",
    "input_line = input_line.lower()\n",
    "\n",
    "#tokenise\n",
    "input_tokens = word_tokenize(input_line)\n",
    "\n",
    "content_words = [word for word in input_tokens dri if word not in stop_words] #Removing Stopwords\n",
    "\n",
    "print(content_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted N-Gram Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc():\n",
    "    words = []\n",
    "    sent_words = []\n",
    "    sent_dict = []\n",
    "    sent_word_dict_array = []\n",
    "    word_set = {}\n",
    "    i = 0\n",
    "    with open(\"tm_data/source_text.txt\") as source_file:\n",
    "        sents = source_file.read().splitlines()\n",
    "        \n",
    "    for sent in sents:\n",
    "        sent = sent.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        words = [x for x in words if x != '.']\n",
    "        word_set = set(word_set).union(words)\n",
    "        sent_words.append(words)\n",
    "        \n",
    "    for words_array in sent_words:\n",
    "        sent_word_dict_array.append(dict.fromkeys(word_set, 0))\n",
    "    \n",
    "    for sent in sent_words:\n",
    "        for word in sent:\n",
    "            sent_word_dict_array[i][word] += 1\n",
    "        i += 1\n",
    "            \n",
    "            \n",
    "    del sent_word_dict_array[0]\n",
    "    return sent_word_dict_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array = get_doc()\n",
    "# import pandas as pd\n",
    "# pd.DataFrame(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(docList):\n",
    "    \"\"\"\n",
    "    idf = ln(total number of docs/number of docs with word in it)\n",
    "    in our case, docs are sentences\n",
    "    \"\"\"\n",
    "    e = 0.00000000000001\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "            idfDict[word] = math.log10(N / (float(val) + e))\n",
    "        \n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(computeIDF(get_doc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams_mtbt(): \n",
    "    \"\"\"\n",
    "    sentence to be translated in MTBT\n",
    "    \"\"\"\n",
    "    sentence = \"There are a few controversies surrounding the the company may keep changing its business strategy topic how many songs did Rafi sing during his lifetime\"\n",
    "    ngrams_list_sent = []\n",
    "    M_ngrams = []\n",
    "    counter_ngrams = []\n",
    "    \n",
    "    ngrams = list(nltk.ngrams(sentence.split(), 4))\n",
    "    ngrams_list_sent.append(list(ngrams))\n",
    "    M_ngrams = [y for x in ngrams_list_sent for y in x]\n",
    "    \n",
    "    for ngrams in M_ngrams:\n",
    "        counter_ngrams.append(Counter(ngrams))\n",
    "        \n",
    "    return M_ngrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams_candidates(candidate_sentence):\n",
    "    ngrams_list_sent = []\n",
    "    C_ngrams = []\n",
    "    counter_ngrams = []\n",
    "    \n",
    "    candidate_sentence = candidate_sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    ngrams = list(nltk.ngrams(candidate_sentence.split(), 4))\n",
    "    ngrams_list_sent.append(list(ngrams))\n",
    "    C_ngrams = [y for x in ngrams_list_sent for y in x]\n",
    "    ngrams_sents = []\n",
    "    ngrams_list_sent = []\n",
    "    \n",
    "    for ngrams in C_ngrams:\n",
    "        counter_ngrams.append(Counter(ngrams))\n",
    "    \n",
    "    return C_ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ngrams_candidates(\"The company may keep changing its business strategy in a steady pace to adapt to the pressure and competition.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_intersection(candidate_sentence):\n",
    "    M_ngrams = get_ngrams_mtbt()\n",
    "    C_ngrams = get_ngrams_candidates(candidate_sentence)\n",
    "    \n",
    "    M_set = set(M_ngrams)\n",
    "    C_set = set(C_ngrams)\n",
    "    \n",
    "    return list(M_set & C_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_w_sum(ngrams_list):\n",
    "    sent_word_dict_array = get_doc()\n",
    "    idfs = computeIDF(sent_word_dict_array)\n",
    "    w = 0\n",
    "    \n",
    "    for ngram in ngrams_list:\n",
    "        for token in ngram:\n",
    "            w+= idfs[token]            \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152.98964151085798"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_ngrams = get_ngrams_mtbt()\n",
    "\n",
    "compute_w_sum(M_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wpn(candidate_sentence):\n",
    "    M_ngrams = get_ngrams_mtbt()\n",
    "    C_ngrams = get_ngrams_candidates(candidate_sentence)\n",
    "    intersection_ngrams = ngrams_intersection(candidate_sentence)\n",
    "    Z = 0.75\n",
    "    \n",
    "    w_M_ngrams = compute_w_sum(M_ngrams)\n",
    "    w_C_ngrams = compute_w_sum(C_ngrams)\n",
    "    w_intersection_ngrams = compute_w_sum(intersection_ngrams)\n",
    "    \n",
    "    \n",
    "    wpn = w_intersection_ngrams / ((Z*w_M_ngrams) + ((1-Z)*w_C_ngrams))\n",
    "    \n",
    "    return wpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_wpn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.562765780206217\n",
      "There are a few controversies surrounding the topic how many songs did Rafi sing during his lifetime \n"
     ]
    }
   ],
   "source": [
    "max_wpn = 0\n",
    "with open(\"tm_data/source_text.txt\") as source_file:\n",
    "    sentences = source_file.read().splitlines()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "        wpn = compute_wpn(sentence)\n",
    "        if wpn > max_wpn:\n",
    "            max_wpn = wpn\n",
    "            best_sentence = sentence\n",
    "            \n",
    "        \n",
    "    print(max_wpn)\n",
    "    print(best_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
